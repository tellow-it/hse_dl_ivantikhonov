{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10898237,"sourceType":"datasetVersion","datasetId":6772890}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers.\n- Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n* Напишите класс для Dataset/Dataloder и азбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n* Попробуйте несколько моделей: Bert, Albert, Deberta. (3 балла) Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json","metadata":{}},{"cell_type":"markdown","source":"## Загрузка и разбиением датасета","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T18:38:28.190945Z","iopub.execute_input":"2025-03-03T18:38:28.191223Z","iopub.status.idle":"2025-03-03T18:38:32.841400Z","shell.execute_reply.started":"2025-03-03T18:38:28.191194Z","shell.execute_reply":"2025-03-03T18:38:32.840622Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/all-accent/all_accents.tsv\", sep=\"\\t\", header=None, names=[\"word\", \"accent\"])\ndf = df.sample(n=400000, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:07:44.897849Z","iopub.execute_input":"2025-03-03T19:07:44.898220Z","iopub.status.idle":"2025-03-03T19:07:48.814594Z","shell.execute_reply.started":"2025-03-03T19:07:44.898194Z","shell.execute_reply":"2025-03-03T19:07:48.813897Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:07:48.815706Z","iopub.execute_input":"2025-03-03T19:07:48.815930Z","iopub.status.idle":"2025-03-03T19:07:48.825547Z","shell.execute_reply.started":"2025-03-03T19:07:48.815910Z","shell.execute_reply":"2025-03-03T19:07:48.824858Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"                        word                accent\n1484615            теплохода            теплох^ода\n392900             задавалам            задав^алам\n988387         перерождающая        перерожд^ающая\n724329          намолотивший         намолот^ивший\n1653846  эволюционировавшими  эволюцион^ировавшими\n...                      ...                   ...\n158688      виртуализируемся     виртуализ^ируемся\n1657121            экранными            экр^анными\n1154810       приготовляются       приготовл^яются\n1154880         пригребалась         пригреб^алась\n1264022         разжигающему         разжиг^ающему\n\n[400000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>accent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1484615</th>\n      <td>теплохода</td>\n      <td>теплох^ода</td>\n    </tr>\n    <tr>\n      <th>392900</th>\n      <td>задавалам</td>\n      <td>задав^алам</td>\n    </tr>\n    <tr>\n      <th>988387</th>\n      <td>перерождающая</td>\n      <td>перерожд^ающая</td>\n    </tr>\n    <tr>\n      <th>724329</th>\n      <td>намолотивший</td>\n      <td>намолот^ивший</td>\n    </tr>\n    <tr>\n      <th>1653846</th>\n      <td>эволюционировавшими</td>\n      <td>эволюцион^ировавшими</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>158688</th>\n      <td>виртуализируемся</td>\n      <td>виртуализ^ируемся</td>\n    </tr>\n    <tr>\n      <th>1657121</th>\n      <td>экранными</td>\n      <td>экр^анными</td>\n    </tr>\n    <tr>\n      <th>1154810</th>\n      <td>приготовляются</td>\n      <td>приготовл^яются</td>\n    </tr>\n    <tr>\n      <th>1154880</th>\n      <td>пригребалась</td>\n      <td>пригреб^алась</td>\n    </tr>\n    <tr>\n      <th>1264022</th>\n      <td>разжигающему</td>\n      <td>разжиг^ающему</td>\n    </tr>\n  </tbody>\n</table>\n<p>400000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"train_words, test_words, train_accent, test_accent = train_test_split(df['word'], df['accent'], test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:07:48.826870Z","iopub.execute_input":"2025-03-03T19:07:48.827164Z","iopub.status.idle":"2025-03-03T19:07:48.971478Z","shell.execute_reply.started":"2025-03-03T19:07:48.827131Z","shell.execute_reply":"2025-03-03T19:07:48.970543Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"train_words.shape, test_words.shape, train_accent.shape, test_accent.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:07:48.972559Z","iopub.execute_input":"2025-03-03T19:07:48.972893Z","iopub.status.idle":"2025-03-03T19:07:48.978584Z","shell.execute_reply.started":"2025-03-03T19:07:48.972861Z","shell.execute_reply":"2025-03-03T19:07:48.977750Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"((200000,), (200000,), (200000,), (200000,))"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"longest_word = max(df[\"word\"], key=len)\nprint(f\"Слово: {longest_word} , Длинна: {len(longest_word)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:07:50.317643Z","iopub.execute_input":"2025-03-03T19:07:50.317984Z","iopub.status.idle":"2025-03-03T19:07:50.425169Z","shell.execute_reply.started":"2025-03-03T19:07:50.317958Z","shell.execute_reply":"2025-03-03T19:07:50.424344Z"}},"outputs":[{"name":"stdout","text":"Слово: лланвайрпуллгуингиллгогерихуирндробуллллантисилиогогогох , Длинна: 56\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"class AccentDataset(Dataset):\n    def __init__(self, words, accent_words, tokenizer, max_length=64):\n        self.words = words\n        self.accent_words = accent_words\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.words)\n\n    def __getitem__(self, idx):\n        word = self.words[idx]\n        accent_word = self.accent_words[idx]\n        \n        encoded = self.tokenizer(word, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        labels = [0] * self.max_length\n        \n        for i, (c1, c2) in enumerate(zip(word, accent_word)):\n            if c1 != c2:\n                labels[i] = 1\n                break\n        \n        return {\n            'input_ids': encoded['input_ids'].squeeze(0),\n            'attention_mask': encoded['attention_mask'].squeeze(0),\n            'labels': torch.tensor(labels, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:00.970041Z","iopub.execute_input":"2025-03-03T19:08:00.970333Z","iopub.status.idle":"2025-03-03T19:08:00.976520Z","shell.execute_reply.started":"2025-03-03T19:08:00.970311Z","shell.execute_reply":"2025-03-03T19:08:00.975422Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"## Тестирование разных моделей","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer, TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:35.968990Z","iopub.execute_input":"2025-03-03T19:08:35.969288Z","iopub.status.idle":"2025-03-03T19:08:35.973218Z","shell.execute_reply.started":"2025-03-03T19:08:35.969264Z","shell.execute_reply":"2025-03-03T19:08:35.972208Z"}},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"## Bert Albert Debert","metadata":{}},{"cell_type":"code","source":"model_names = [\"DeepPavlov/rubert-base-cased\", \"albert-base-v2\", \"microsoft/deberta-v3-base\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:36.843015Z","iopub.execute_input":"2025-03-03T19:08:36.843347Z","iopub.status.idle":"2025-03-03T19:08:36.847177Z","shell.execute_reply.started":"2025-03-03T19:08:36.843317Z","shell.execute_reply":"2025-03-03T19:08:36.846194Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n\n    mask = labels != -100\n    predictions = predictions[mask]\n    labels = labels[mask]\n\n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\")\n\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:40.253240Z","iopub.execute_input":"2025-03-03T19:08:40.253593Z","iopub.status.idle":"2025-03-03T19:08:40.258478Z","shell.execute_reply.started":"2025-03-03T19:08:40.253562Z","shell.execute_reply":"2025-03-03T19:08:40.257596Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"def train_model(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    train_dataset = AccentDataset(train_words.tolist(), train_accent.tolist(), tokenizer)\n    test_dataset = AccentDataset(test_words.tolist(), test_accent.tolist(), tokenizer)\n    \n    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        fp16=True,\n        logging_steps=10,\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        processing_class=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    metrics = trainer.evaluate() \n    return tokenizer, model, metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:42.579403Z","iopub.execute_input":"2025-03-03T19:08:42.579795Z","iopub.status.idle":"2025-03-03T19:08:42.585222Z","shell.execute_reply.started":"2025-03-03T19:08:42.579765Z","shell.execute_reply":"2025-03-03T19:08:42.584252Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"trained_models = {}\ntokenizer_models = {}\nmodel_metrics = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:42.917499Z","iopub.execute_input":"2025-03-03T19:08:42.917792Z","iopub.status.idle":"2025-03-03T19:08:42.921292Z","shell.execute_reply.started":"2025-03-03T19:08:42.917769Z","shell.execute_reply":"2025-03-03T19:08:42.920490Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"for model_name in model_names:\n    print(f\"Training {model_name}...\")\n    tokenizer_models[model_name], trained_models[model_name], model_metrics[model_name] = train_model(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T19:08:44.696457Z","iopub.execute_input":"2025-03-03T19:08:44.696810Z","iopub.status.idle":"2025-03-04T00:21:40.853988Z","shell.execute_reply.started":"2025-03-03T19:08:44.696780Z","shell.execute_reply":"2025-03-04T00:21:40.852926Z"}},"outputs":[{"name":"stdout","text":"Training DeepPavlov/rubert-base-cased...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18750/18750 1:28:29, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.023100</td>\n      <td>0.021278</td>\n      <td>0.991480</td>\n      <td>0.901200</td>\n      <td>0.796513</td>\n      <td>0.840735</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014000</td>\n      <td>0.014621</td>\n      <td>0.994429</td>\n      <td>0.922307</td>\n      <td>0.890771</td>\n      <td>0.905906</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.010300</td>\n      <td>0.013644</td>\n      <td>0.995331</td>\n      <td>0.930976</td>\n      <td>0.914570</td>\n      <td>0.922608</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 06:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training albert-base-v2...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18750/18750 1:27:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.049800</td>\n      <td>0.050043</td>\n      <td>0.984380</td>\n      <td>0.492190</td>\n      <td>0.500000</td>\n      <td>0.496064</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.049800</td>\n      <td>0.050008</td>\n      <td>0.984380</td>\n      <td>0.492190</td>\n      <td>0.500000</td>\n      <td>0.496064</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.050500</td>\n      <td>0.049980</td>\n      <td>0.984380</td>\n      <td>0.492190</td>\n      <td>0.500000</td>\n      <td>0.496064</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 07:41]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Training microsoft/deberta-v3-base...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18750/18750 1:52:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.025900</td>\n      <td>0.023884</td>\n      <td>0.993437</td>\n      <td>0.916966</td>\n      <td>0.858067</td>\n      <td>0.885201</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.021900</td>\n      <td>0.020867</td>\n      <td>0.994477</td>\n      <td>0.938226</td>\n      <td>0.873181</td>\n      <td>0.903009</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.018200</td>\n      <td>0.020071</td>\n      <td>0.994889</td>\n      <td>0.945850</td>\n      <td>0.880161</td>\n      <td>0.910307</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 08:42]\n    </div>\n    "},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"for model, metrics in model_metrics.items():\n    print(f\"Metrics {model_name}...\")\n    print(f\"{model}: {metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T00:21:40.855235Z","iopub.execute_input":"2025-03-04T00:21:40.855522Z","iopub.status.idle":"2025-03-04T00:21:40.861679Z","shell.execute_reply.started":"2025-03-04T00:21:40.855500Z","shell.execute_reply":"2025-03-04T00:21:40.860740Z"}},"outputs":[{"name":"stdout","text":"Metrics microsoft/deberta-v3-base...\nDeepPavlov/rubert-base-cased: {'eval_loss': 0.013644049875438213, 'eval_accuracy': 0.995330546875, 'eval_precision': 0.9309755488075154, 'eval_recall': 0.9145696458884888, 'eval_f1': 0.9226078621069406, 'eval_runtime': 413.2664, 'eval_samples_per_second': 483.949, 'eval_steps_per_second': 15.123, 'epoch': 3.0}\nMetrics microsoft/deberta-v3-base...\nalbert-base-v2: {'eval_loss': 0.049980439245700836, 'eval_accuracy': 0.9843796875, 'eval_precision': 0.49218984375, 'eval_recall': 0.5, 'eval_f1': 0.49606418252555307, 'eval_runtime': 474.3702, 'eval_samples_per_second': 421.612, 'eval_steps_per_second': 13.175, 'epoch': 3.0}\nMetrics microsoft/deberta-v3-base...\nmicrosoft/deberta-v3-base: {'eval_loss': 0.02007102221250534, 'eval_accuracy': 0.994889296875, 'eval_precision': 0.9458500941134138, 'eval_recall': 0.8801612871969267, 'eval_f1': 0.9103067739677138, 'eval_runtime': 541.4066, 'eval_samples_per_second': 369.408, 'eval_steps_per_second': 11.544, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"for model_name, tokenizer in tokenizer_models.items():\n    test_word = \"корова\"\n\n    model = trained_models[model_name].to(device)\n\n    inputs = tokenizer(test_word, padding='max_length', truncation=True, max_length=64, return_tensors='pt').to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        \n    predictions = torch.argmax(outputs.logits, dim=-1).cpu()\n\n    print(f\"Prediction of model {model_name}: {predictions}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T00:21:40.863416Z","iopub.execute_input":"2025-03-04T00:21:40.863625Z","iopub.status.idle":"2025-03-04T00:21:40.936183Z","shell.execute_reply.started":"2025-03-04T00:21:40.863607Z","shell.execute_reply":"2025-03-04T00:21:40.935546Z"}},"outputs":[{"name":"stdout","text":"Prediction of model DeepPavlov/rubert-base-cased: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nPrediction of model albert-base-v2: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nPrediction of model microsoft/deberta-v3-base: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}